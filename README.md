# tricks-used-in-deep-learning
Tricks used in deep learning. Including papers read recently.


## Improving softmax

Gumbel-Softmax: [Categorical Reparameterization with Gumbel-Softmax](https://arxiv.org/abs/1611.01144)

Confidence penalty: [Regularizing Neural Networks by Penalizing Confident Output Distributions](https://arxiv.org/abs/1701.06548)

## Normalization

weight normalization: [Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks](https://arxiv.org/abs/1602.07868)

Batch Renormalization: [Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models](https://arxiv.org/abs/1702.03275)

## Weight compressing

[Soft weight-sharing for Neural Network compression](https://arxiv.org/abs/1702.04008)

## GAN

[GAN tricks](https://github.com/soumith/ganhacks)

[Wasserstein GAN](https://arxiv.org/abs/1701.07875):
[Example on MNIST](https://gist.github.com/f0k/f3190ebba6c53887d598d03119ca2066)

[Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities](https://arxiv.org/abs/1701.06264)
