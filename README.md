# tricks-used-in-deep-learning
Tricks used in deep learning. Including papers read recently.

## Improving softmax

Gumbel-Softmax: [Categorical Reparameterization with Gumbel-Softmax](https://arxiv.org/abs/1611.01144)

Confidence penalty: [Regularizing Neural Networks by Penalizing Confident Output Distributions](https://arxiv.org/abs/1701.06548)

## Normalization

weight normalization: [Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks](https://arxiv.org/abs/1602.07868)

Batch Renormalization: [Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models](https://arxiv.org/abs/1702.03275)

## Weight compressing

[Soft weight-sharing for Neural Network compression](https://arxiv.org/abs/1702.04008)

## GAN

[GAN tricks](https://github.com/soumith/ganhacks)

[Wasserstein GAN](https://arxiv.org/abs/1701.07875):[my implementation](https://github.com/bobchennan/Wasserstein-GAN-Keras)
[Example on MNIST](https://gist.github.com/f0k/f3190ebba6c53887d598d03119ca2066)

[Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities](https://arxiv.org/abs/1701.06264)


## Matrix Factorization

[MCPCA](https://arxiv.org/abs/1702.05471v1)


## Feature representation
[Attentive Recurrent Comparators](https://arxiv.org/abs/1703.00767)
[code](https://github.com/pranv/ARC)


## Training
[Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343)

## Dropout
[Variational Dropout Sparsifies Deep Neural Networks](https://arxiv.org/abs/1701.05369)
[code](https://github.com/ars-ashuha/variational-dropout-sparsifies-dnn)

[Concrete Dropout](https://arxiv.org/abs/1705.07832)


## Transfer Learning
[Sobolev Training for Neural Networks](https://arxiv.org/abs/1706.04859)


## Face Recognition
[ArcFace](https://arxiv.org/abs/1801.07698)


## Adaptation
[Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)


## Data Augmentation
[mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)

[Random Erasing Data Augmentation](https://arxiv.org/abs/1708.04896)
